{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ffffe4",
   "metadata": {},
   "source": [
    "# K-Means Clustering: Theory and Fundamentals\n",
    "\n",
    "Welcome to the first notebook in our K-Means clustering series! In this notebook, we'll cover:\n",
    "\n",
    "1. What is clustering and unsupervised learning?\n",
    "2. The K-Means algorithm (Lloyd's algorithm)\n",
    "3. Mathematical foundations\n",
    "4. Choosing the optimal number of clusters (K)\n",
    "5. Assumptions, pitfalls, and limitations\n",
    "\n",
    "By the end of this notebook, you'll have a solid theoretical understanding of K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46462ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000359a",
   "metadata": {},
   "source": [
    "## 1. What is Clustering?\n",
    "\n",
    "### Machine Learning Paradigms\n",
    "\n",
    "Machine learning can be broadly categorized into:\n",
    "\n",
    "**Supervised Learning:**\n",
    "- You have labeled data (input â†’ output pairs)\n",
    "- Goal: Learn to predict labels for new data\n",
    "- Examples: Email spam detection, house price prediction, image classification\n",
    "\n",
    "**Unsupervised Learning:**\n",
    "- You have unlabeled data (just inputs, no outputs)\n",
    "- Goal: Discover hidden patterns or structure in the data\n",
    "- Examples: Customer segmentation, anomaly detection, data compression\n",
    "\n",
    "**Clustering** is the most common unsupervised learning task:\n",
    "> *Group similar data points together, where points in the same group are more similar to each other than to points in other groups.*\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "| Domain | Application | Why Clustering? |\n",
    "|--------|-------------|----------------|\n",
    "| **Marketing** | Customer segmentation | Target different groups with personalized campaigns |\n",
    "| **Biology** | Gene expression analysis | Find groups of genes with similar behavior |\n",
    "| **Computer Vision** | Image segmentation | Separate objects from background |\n",
    "| **E-commerce** | Product recommendations | Group similar products or users |\n",
    "| **Security** | Anomaly detection | Identify unusual patterns in network traffic |\n",
    "| **Data Compression** | Reduce data size | Store only cluster representatives |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee5982",
   "metadata": {},
   "source": [
    "## 2. The K-Means Algorithm\n",
    "\n",
    "K-Means is one of the simplest and most popular clustering algorithms. The name comes from:\n",
    "- **K**: The number of clusters (you choose this)\n",
    "- **Means**: Each cluster is represented by the mean (average) of its points\n",
    "\n",
    "### Lloyd's Algorithm (The Standard K-Means)\n",
    "\n",
    "**Input:** \n",
    "- Dataset $X = \\{x_1, x_2, ..., x_n\\}$ where $x_i \\in \\mathbb{R}^d$\n",
    "- Number of clusters $K$\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. **Initialize:** Randomly select $K$ points as initial centroids $\\mu_1, \\mu_2, ..., \\mu_K$\n",
    "\n",
    "2. **Repeat until convergence:**\n",
    "   \n",
    "   **Assignment Step:** Assign each point to the nearest centroid\n",
    "   $$C_i = \\{x_p : \\|x_p - \\mu_i\\| \\leq \\|x_p - \\mu_j\\| \\text{ for all } j\\}$$\n",
    "   \n",
    "   **Update Step:** Recompute centroids as the mean of assigned points\n",
    "   $$\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x$$\n",
    "\n",
    "3. **Output:** Final clusters $C_1, C_2, ..., C_K$ and centroids $\\mu_1, \\mu_2, ..., \\mu_K$\n",
    "\n",
    "### Convergence Criterion\n",
    "\n",
    "The algorithm stops when:\n",
    "- Centroids no longer move (or move very little)\n",
    "- Cluster assignments don't change\n",
    "- Maximum number of iterations reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for visualization\n",
    "np.random.seed(42)\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, alpha=0.6, edgecolors='black', linewidths=0.5)\n",
    "plt.title('Unlabeled Data: Can you see the clusters?', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0417fd4",
   "metadata": {},
   "source": [
    "### Simple Animation: How K-Means Works\n",
    "\n",
    "Let's visualize how centroids move during the algorithm's iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_step_by_step(X, K, max_iters=10, random_state=42):\n",
    "    \"\"\"Perform K-Means step by step for visualization.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Initialize centroids randomly\n",
    "    indices = np.random.choice(len(X), K, replace=False)\n",
    "    centroids = X[indices].copy()\n",
    "    \n",
    "    history = [(centroids.copy(), None)]  # Store (centroids, labels)\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        # Assignment step\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        history.append((centroids.copy(), labels.copy()))\n",
    "        \n",
    "        # Update step\n",
    "        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.allclose(centroids, new_centroids, rtol=1e-4):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    history.append((centroids.copy(), labels.copy()))\n",
    "    return history\n",
    "\n",
    "# Run K-Means step by step\n",
    "K = 4\n",
    "history = kmeans_step_by_step(X, K, max_iters=10)\n",
    "\n",
    "print(f\"âœ… K-Means converged in {len(history)-1} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize iterations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, K))\n",
    "\n",
    "for idx, (centroids, labels) in enumerate(history[:6]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if labels is None:\n",
    "        # Initial state - no assignments yet\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=50, alpha=0.3, c='gray', edgecolors='black', linewidths=0.5)\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', \n",
    "                   edgecolors='black', linewidths=2, label='Initial Centroids')\n",
    "        ax.set_title(f'Iteration 0: Random Initialization', fontsize=12, fontweight='bold')\n",
    "    else:\n",
    "        # Assigned clusters\n",
    "        for k in range(K):\n",
    "            cluster_points = X[labels == k]\n",
    "            ax.scatter(cluster_points[:, 0], cluster_points[:, 1], s=50, \n",
    "                       c=[colors[k]], alpha=0.6, edgecolors='black', linewidths=0.5)\n",
    "        \n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], s=300, c='black', marker='X', \n",
    "                   edgecolors='white', linewidths=2, label='Centroids')\n",
    "        ax.set_title(f'Iteration {idx}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ‘† Notice how centroids move toward the center of their assigned clusters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e56dfd",
   "metadata": {},
   "source": [
    "## 3. Mathematical Foundations\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "K-Means aims to minimize the **Within-Cluster Sum of Squares (WCSS)**, also called **inertia**:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{K} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $J$ = total WCSS (lower is better)\n",
    "- $K$ = number of clusters\n",
    "- $C_i$ = set of points in cluster $i$\n",
    "- $x$ = a data point\n",
    "- $\\mu_i$ = centroid of cluster $i$\n",
    "- $\\|x - \\mu_i\\|^2$ = squared Euclidean distance\n",
    "\n",
    "**Intuition:** We want points to be as close as possible to their cluster's centroid.\n",
    "\n",
    "### Convergence Guarantee\n",
    "\n",
    "**Theorem:** Lloyd's algorithm converges to a local minimum of the WCSS objective.\n",
    "\n",
    "**Proof sketch:**\n",
    "1. Each assignment step decreases or maintains WCSS (we assign each point to the nearest centroid)\n",
    "2. Each update step decreases or maintains WCSS (the mean minimizes squared distances)\n",
    "3. WCSS is bounded below by 0\n",
    "4. Therefore, the algorithm must converge\n",
    "\n",
    "**Important caveat:** K-Means converges to a **local** minimum, not necessarily the **global** minimum. Different initializations can lead to different solutions!\n",
    "\n",
    "### Distance Metric\n",
    "\n",
    "Standard K-Means uses **Euclidean distance**:\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sqrt{\\sum_{j=1}^{d} (x_j - y_j)^2}\n",
    "$$\n",
    "\n",
    "where $d$ is the number of features.\n",
    "\n",
    "**Alternative distances** (less common):\n",
    "- Manhattan distance: $d(x, y) = \\sum_{j=1}^{d} |x_j - y_j|$\n",
    "- Cosine distance: $d(x, y) = 1 - \\frac{x \\cdot y}{\\|x\\| \\|y\\|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate WCSS calculation\n",
    "def calculate_wcss(X, labels, centroids):\n",
    "    \"\"\"Calculate Within-Cluster Sum of Squares.\"\"\"\n",
    "    wcss = 0\n",
    "    for k in range(len(centroids)):\n",
    "        cluster_points = X[labels == k]\n",
    "        wcss += np.sum((cluster_points - centroids[k]) ** 2)\n",
    "    return wcss\n",
    "\n",
    "# Calculate WCSS for each iteration\n",
    "wcss_per_iteration = []\n",
    "for centroids, labels in history:\n",
    "    if labels is not None:\n",
    "        wcss = calculate_wcss(X, labels, centroids)\n",
    "        wcss_per_iteration.append(wcss)\n",
    "\n",
    "# Plot WCSS convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(wcss_per_iteration)), wcss_per_iteration, \n",
    "         marker='o', linewidth=2, markersize=8, color='#e74c3c')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\n",
    "plt.title('K-Means Objective Function Convergence', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial WCSS: {wcss_per_iteration[0]:.2f}\")\n",
    "print(f\"Final WCSS: {wcss_per_iteration[-1]:.2f}\")\n",
    "print(f\"Reduction: {(1 - wcss_per_iteration[-1]/wcss_per_iteration[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c961e",
   "metadata": {},
   "source": [
    "## 4. Choosing the Optimal K\n",
    "\n",
    "One of the biggest challenges with K-Means: **you must specify K in advance!**\n",
    "\n",
    "### Method 1: The Elbow Method\n",
    "\n",
    "**Idea:** Run K-Means for different values of K and plot the WCSS.\n",
    "\n",
    "- As K increases, WCSS always decreases (more clusters = better fit)\n",
    "- But the rate of decrease slows down after a certain point\n",
    "- The \"elbow\" in the curve suggests the optimal K\n",
    "\n",
    "**How to identify the elbow:**\n",
    "- Look for the point where the curve bends sharply\n",
    "- After the elbow, adding more clusters doesn't help much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as SklearnKMeans\n",
    "\n",
    "# Test different values of K\n",
    "K_range = range(1, 11)\n",
    "wcss_values = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = SklearnKMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    wcss_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, wcss_values, marker='o', linewidth=2, markersize=10, color='#3498db')\n",
    "plt.axvline(x=4, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Optimal K=4 (elbow)')\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)', fontsize=12)\n",
    "plt.title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Look for the 'elbow' where the curve bends!\")\n",
    "print(\"In this case, K=4 seems optimal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f546e0",
   "metadata": {},
   "source": [
    "### Method 2: Silhouette Score\n",
    "\n",
    "**Idea:** Measure how similar each point is to its own cluster compared to other clusters.\n",
    "\n",
    "For each point $i$:\n",
    "\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $a(i)$ = average distance to other points in the same cluster\n",
    "- $b(i)$ = average distance to points in the nearest different cluster\n",
    "\n",
    "**Interpretation:**\n",
    "- $s(i) \\approx 1$: Point is well-clustered (far from other clusters)\n",
    "- $s(i) \\approx 0$: Point is on the border between clusters\n",
    "- $s(i) \\approx -1$: Point might be in the wrong cluster\n",
    "\n",
    "**Average silhouette score** across all points: higher is better!\n",
    "\n",
    "**Typical range:** -1 to +1\n",
    "- > 0.7: Strong structure\n",
    "- 0.5 - 0.7: Reasonable structure\n",
    "- < 0.5: Weak or artificial structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate silhouette scores for different K\n",
    "K_range_sil = range(2, 11)  # Silhouette needs at least 2 clusters\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range_sil:\n",
    "    kmeans = SklearnKMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range_sil, silhouette_scores, marker='o', linewidth=2, markersize=10, color='#2ecc71')\n",
    "plt.axvline(x=4, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Optimal K=4')\n",
    "plt.axhline(y=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "plt.xlabel('Number of Clusters (K)', fontsize=12)\n",
    "plt.ylabel('Average Silhouette Score', fontsize=12)\n",
    "plt.title('Silhouette Score Analysis', fontsize=14, fontweight='bold')\n",
    "plt.xticks(K_range_sil)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = list(K_range_sil)[np.argmax(silhouette_scores)]\n",
    "print(f\"ðŸ“ˆ Best K according to silhouette score: {best_k}\")\n",
    "print(f\"Silhouette score at K={best_k}: {max(silhouette_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260d221",
   "metadata": {},
   "source": [
    "### Method 3: Domain Knowledge\n",
    "\n",
    "Sometimes the best choice of K comes from your understanding of the problem:\n",
    "\n",
    "- **Marketing:** \"We want 3-5 customer segments for our campaign\"\n",
    "- **Biology:** \"We expect 4 cell types based on prior research\"\n",
    "- **Image compression:** \"Use 16 colors for reasonable quality\"\n",
    "\n",
    "**Best practice:** Combine multiple methods!\n",
    "1. Start with elbow + silhouette to get candidates (e.g., K=3, 4, or 5)\n",
    "2. Visualize the clusters for each candidate\n",
    "3. Choose based on domain knowledge and business goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5788b",
   "metadata": {},
   "source": [
    "## 5. Assumptions, Pitfalls, and Limitations\n",
    "\n",
    "### Assumptions of K-Means\n",
    "\n",
    "K-Means works best when:\n",
    "\n",
    "1. **Clusters are spherical (circular/ball-shaped)**\n",
    "   - Uses Euclidean distance, which assumes equal spread in all directions\n",
    "   \n",
    "2. **Clusters have similar sizes**\n",
    "   - One large cluster and one small cluster can cause problems\n",
    "   \n",
    "3. **Clusters have similar densities**\n",
    "   - Dense vs. sparse clusters can be split incorrectly\n",
    "   \n",
    "4. **Features are on similar scales**\n",
    "   - Always standardize/normalize your data first!\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "#### 1. Sensitivity to Initialization\n",
    "- Random initialization can lead to different results\n",
    "- **Solution:** Use K-Means++ initialization (covered in next notebook)\n",
    "\n",
    "#### 2. Outliers\n",
    "- Outliers can pull centroids away from true cluster centers\n",
    "- **Solution:** Remove outliers or use robust clustering methods\n",
    "\n",
    "#### 3. Non-spherical Clusters\n",
    "- K-Means fails with elongated or irregular shapes\n",
    "- **Solution:** Use DBSCAN, Gaussian Mixture Models, or spectral clustering\n",
    "\n",
    "#### 4. Curse of Dimensionality\n",
    "- In high dimensions, Euclidean distance becomes less meaningful\n",
    "- **Solution:** Apply PCA or UMAP for dimensionality reduction first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33632156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate failure case: Non-spherical clusters\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans_moons = SklearnKMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_moons = kmeans_moons.fit_predict(X_moons)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True structure\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], s=50, alpha=0.6, edgecolors='black', linewidths=0.5)\n",
    "axes[0].set_title('Non-Spherical Clusters (True Structure)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# K-Means result\n",
    "colors = ['#1f77b4', '#ff7f0e']\n",
    "for k in range(2):\n",
    "    cluster_points = X_moons[labels_moons == k]\n",
    "    axes[1].scatter(cluster_points[:, 0], cluster_points[:, 1], s=50, \n",
    "                    c=colors[k], alpha=0.6, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "axes[1].scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1],\n",
    "                s=300, c='black', marker='X', edgecolors='white', linewidths=2, label='Centroids')\n",
    "axes[1].set_title('K-Means Result (Incorrect!)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âš ï¸ K-Means fails when clusters are not spherical!\")\n",
    "print(\"For such data, consider DBSCAN or spectral clustering instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bb3d3",
   "metadata": {},
   "source": [
    "### When to Use K-Means\n",
    "\n",
    "âœ… **Good for:**\n",
    "- Large datasets (fast and scalable)\n",
    "- Exploratory data analysis\n",
    "- When you have a rough idea of K\n",
    "- Spherical, well-separated clusters\n",
    "- Customer segmentation, image compression, data preprocessing\n",
    "\n",
    "âŒ **Not good for:**\n",
    "- Non-spherical clusters (use DBSCAN, spectral clustering)\n",
    "- Varying cluster densities (use DBSCAN)\n",
    "- Unknown K with no clear elbow (use hierarchical clustering)\n",
    "- High-dimensional sparse data (apply dimensionality reduction first)\n",
    "- Presence of strong outliers (preprocess or use robust methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f1a8a1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. âœ… **What is clustering?** Unsupervised learning to group similar data\n",
    "2. âœ… **Lloyd's algorithm:** Assignment â†’ Update â†’ Repeat\n",
    "3. âœ… **Mathematical foundation:** Minimize WCSS, convergence to local minimum\n",
    "4. âœ… **Choosing K:** Elbow method, silhouette score, domain knowledge\n",
    "5. âœ… **Limitations:** Sensitive to initialization, assumes spherical clusters, affected by outliers\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Notebook 02**, we'll:\n",
    "- Implement K-Means from scratch using NumPy\n",
    "- Compare our implementation with scikit-learn\n",
    "- Learn about K-Means++ initialization\n",
    "- Benchmark performance\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to code?** â†’ [02_implementation.ipynb](02_implementation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
